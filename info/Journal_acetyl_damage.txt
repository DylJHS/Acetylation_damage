Prior:
- Collected sample folders containing raw FASTQ files for analysis.
- Assessed sequence quality using FastQC, revealing manageable overall quality, though slight trimming was required due to minor quality issues identified.
- Trimmed adapter sequences and low-quality reads using Trim Galore, resulting in improved overall read quality.

- Initially downloaded the Drosophila melanogaster reference genome (dm6 version) and constructed the Bowtie2 index for aligning sequencing reads.
- Attempted alignments of trimmed FASTQ files to the Drosophila reference genome resulted in unexpectedly low alignment rates (approximately 1-4% across all samples).
- Suspecting an issue with the genome version, a different Drosophila reference genome from Ensembl.org was obtained and indexed, but the alignment performance did not improve significantly.

- Due to persistently poor alignments, decided to investigate the source of sequences by extracting a smaller subset (100-500 reads) from sample FASTQ files and converting them to FASTA format.
- Initial BLAST queries through the NCBI website indicated reads predominantly matched human sequences, along with other organisms such as mouse and fungi, with no significant matches to Drosophila melanogaster.
- Conducted a more comprehensive BLAST analysis locally, building separate BLAST databases for the human and Drosophila genomes.
- BLAST analyses against the human genome consistently showed significantly higher alignment rates (ranging from 2.8% to 70.3%), with millions of hits and high identity percentages averaging around 90%.
- In contrast, BLAST queries against the Drosophila database yielded very low alignment rates (ranging from 0.0% to 0.7%), with minimal numbers of hits.

Summary Interpretation:
The data strongly indicates contamination of the Drosophila sequencing samples with human genomic material, explaining the initially poor alignment rates to the Drosophila reference genome. High human alignment percentages, with substantial average identity scores (~89%-98%) and lower mismatch rates, confirm human DNA as the predominant component in the sequencing data. 

07/05/25:
- Validating the experiment was not run on the human (mostly) samples
- Need to assess the level of acetylation across the samples
- Using the bam files that were generated during the initial analysis 
- removing the duplicates from the files using markdup in Samtools
- "samtools_pcr_duplicate_removal_loop.sh" is the hpc script which loops over the bam fiels and performs the removal

08/05/25:
- Ran the script 
- Debugged the script (mainly conda env issues)
- Got the resulting de-duplicated files but issue with the looping
- modified script to only loop over the original JAN-00?.bam files
- the dedup script removed most of the reads 
- reran with proper fixmate args and get better results

09/05/25: 
- Looking at the resutls from the plotFingerprint from deepTools shows that the controls and the induced samples seem to have the same level of enrichment for the human alignment
- This should not be the case
- the control line and the induced "input" lines should deviate with the induced ones showing increased enrichment (more of an elbow in the plot)
- But not seeing a difference between the induced and control is not indicative of there being an issue 
- It could just mean that the null hypothesis is true and there is no acetylation difference 
- But would still expect a higher curve due to the marks being a broad mark
- Ran the scrtips on the "tagged" bam files which show better results but not sure where they come from and how they were produced
- Will continue the analysis on tagged and untagged and assess the differences
- Generating the bamCoverages for the data

13/05/25
- generating the bigwig files for the fly data
- Checking the coverages in IGV 

14/05/25
- Running the integration_site_test scripts 
- Generating the RPKM bigwig files for better comparison between samples 
- Generating the counts.txt file for the quantitative data in order to test between the samples at the integration int_site
- Need to perform a comparitive test betweent the counts "integration_counts.txt"
- Will perform a Fisher's exact test 

15/05/25
- Ran the Fisher exact test for the coverage from 22245250 to 22245350 (100bp) 
- The test did not reveal anything significant likely due to the high degree of background reads
- Emailed Arianna with some of the findings and the IGV tracks 

25/05/25
- Trying to find out the source of the issue with SCC
- Need to give a recommendation for the sequencing depth 

10/07/25
- Downloaded the newly run data from SCC 
- stored in: P:\Janssen\chet_repair\dhaynessimmons\raw_data\Dros_H3K9ac_bulkChIC_Analysis\SCC-bulkChIC-UMC-JULY.tar.gz

11/07/25
- Unzipping the folders in the Projects dir 

12/07/25
- Sending the new SCC files ot the HPC server to run the analyses
- Need to check the data

14/07/25
- Going over the Dros_H3K9ac_bulkChIC_Analysis scripts and noticed that they need better descriptions and titles (comments are good though)
- Looking first at the BLAST scripts since those are the ones that will be used first
- Combing the BLAST scripts into a single bash script for the hpc server
- The increase in the read depth means that more lanes were used instead of just one
- This may impact the file strucutres and how they are used in the scripts

15/07/25
- Re-extracting the SCC folder since only 4 samples were perviously in the decompressed folder
- All samples are present in the new decompression
- Combining the BLAST scripts all in one
- Running the full script on all 6 samples
- the BLAST results seem to show the same kind of relation to the human genome instead of the drosophila ones
- run again with more stringent evalue 1e-10

16/07/25
- The results from the previous run were better but still showed better correlation with the human genome than the drosophila
- Restricted the parameters more for more stringent query: 
    - task megablast
    - perc_identity = 95
    - ungapped
    - evalue = 1e-20
- Converting the BLASTn query and the FASTA_generation scripts to handle array jobs for faster runtime "BLASTn_arr.sh"
- Still need to create a single launch script (FASTA gen + BLASTn query + results summary file gen)

17/07/25
- Creation of BLAST config file "BLAST_wkflw_config.sh" to reference for all the inner jobs
- Creation of BLAST workflow script to run the full pipeline with all 3 jobs (gen_fasta_subsets_arr.sh, BLASTn_arr.sh & gen_blast_summary.sh) => "run_blast_wkflw.sh"
- Rerun the BLASTn query using the following stringent args: 
    -task megablast \
    -perc_identity 95 \
    -max_target_seqs 1 \
    -max_hsps 1 \
    -ungapped \
    -evalue 1e-20 \
- Still showing better results for the human BLAST query than the fly despite the stricter settings
- Going to trim, qc, align and then see the data
- Ran the multiqc
- High duplication rate means that high pcr bias or oversequencing due to low complexity (common in ChIC seq) 
- Need to deduplicate 
- Higher R1 read duplication than R2 for all fastq files; might be normal due to the cutting process of the Mnase
- MultiQC report duplicate rates are likely higher than the actual duplicate rates due to how they only assess the first 50 bp of the reads 

22/07/25
- Trimming the reads
- Trimmed reads added to the trimmed_fastq folder in the data dir 
- Also added to the config file
- Built the drosophila bowtie2 index using the "fly_build_bowtie2_index.sh" script
- Concatenating the trimmed reads across lanes into single files for each sample
- Need to run MultiQC on the trimmed reads to assess the quality
- Issues running trim galore due to bug when using multiple cores 
- Removing cores to see if it resolves the issue
- Could also be due to how the script runs. Once in single mode and then in paired mode
- After meeting with Aniek need to: 
	- Check the break site for repair at th ISC locus
	- look for deletion (1bp) at the break site (within the integration site)
	- Look at the reads at the break site 

23/07/25
- Changing the trim_fastq_galore.sh script to only run the trim command 
- Need to build new script that then concatenates the trimmed reads across lanes into single files for each sample
- running the fastqc & multiqc commands on the trimmed reads
- Concatenating the trimmed reads across lanes into merged files using concat_fastq.sh script
- results from the trimming and subsequent qc did not show significant improvements
- Rerunning the trim galore script with clip argument on the first 6bp 
- stringency set to 3 for better trimming

24/07/25
- PA-MNase has a bias for A/T rich regions of the genome
- Adds an A base to the start and T to the end
- Re-running the trim galore script with the clip argument set to 10bp and removed the --illumina flag for automatic adapter detection
- Need to trim -> align -> deduplicate -> assess the alignment rates
- Editting the bowtie2 align script to use the new trimmed reads and to run in array mode

25/07/25
- Editting the bowtie2 fly alignment script to use the new trimmed reads and to run in array mode
- Finished generating the multiqc for the merged 10bp clipped reads; the results are similar to the previous runs
- the A/T bias as well as the non-random binding of proteins leads to a non-uniform distribution of bases in the reads
- Running the bowtie2 alignment script on the HPC server
- The drosophila alignement is done 
- Running the human alignment now
- Need to assess the alignment rates usign the samtools flagstat command
- Updated the alignment assessment flagstat script to use the new config file and to run in array mode

26/07/25
- Running the flagstat script for the human alignments and the drosophila alignments
- Runnign the multiqc on the alignment results
- Editting the duplicate removal samtools markdup script to use the new config file and to run in array mode

28/07/25
- Running the samtools markdup script to remove the duplicates from the drosophila
- will run the multiqc on the deduplicated results
- Updating the files on the hpc server due to storage issues
- trying to save the smaller files while keeping the initial raw files as well as all scripts to be able to reproduce the results
- saving the raw fastq files as well as the trimmed fastq files by lane but not the merged files (will need to be regenerated if needed using the concat_fastq.sh script)
- will be saving the fastqc and multiqc reports for the merged files though

29/07/25
- Running the samtools markdup script on the human alignments
- Cleared out a lot of the data from the HPC server to make space for the new files

30/07/25
- Running the multiqc on the deduplicated results for the drosophila deduplicated files
- Results from the dros dedup alignments show that the deduplication was successful but with a near complete reduction in the number of reads
- Rerunning the markdup on the human alignments due to issue with temp files
- Rerunning the markdup for both the human and drosophila alignments with stat output files
- Moved the tagged files to the tagged folder in the data dir for the original files
- Need to run the flagstat on the tagged files to assess the alignment rates
- Stopped the markdup scripts 
- Need to run the markdup scripts one after the other for either species so there is no conflict with the temp files
- Have all the human bowtie alignment flagstat files 
- Do not have the drosophila bowtie alignment flagstat files
- Need to run the flagstat script on the tagged files for drosophila
- Rerunning the flagstat script on the drosophila bowtie alignments
- Rerunning the markdup script on the drosophila bowtie alignments
- Will need to rerun the markdup script on the human bowtie alignments once the drosophila is done
- Looking at the flagstat results from the tagged files which are meant to be aligned to the drosophila genome
    it looks like they are able to get 100% mapped with at least 28M reads; very different from my results;
    missing the flagstat file for JAN-007, so will rerun
- Lost the JAN-004 human bowtie aligned bam file 
- Going to run the markdup on the tagged files 
- Ran the markdup on the tagged files
- Ran the flagstat on the deduplicated tagged files which still left around 80% of most reads
- Not close to the results that we achieved on the raw fastqs
- The trimming is likely causing a better alignement or just leaving more reads behind
- Need to read the documentation for the processing of the data 

31/07/25
- the Core uses the chic snakemake pipeline to process the data (but the specific config file for our data was not supplied)
- the script is info/SCC_ChIC_Snakefile.txt (https://github.com/BuysDB/SingleCellMultiOmics/blob/master/singlecellmultiomics/snakemake_workflows/chic/Snakefile)
- Going to just use the final bam files delivered by the core since their results seem to be better
- Editted the bowtie2 script to match the SCMO script's alignment setup
- Can't rerun the bowtie at teh moment due to having moved the files
- going to run the fingerplotting on the data to see the level of background noise
- slight difference in the deduped vs original tagged bam files

01/08/25
- The bulk and scChIC seq that The Single cell core offer are actually based on the Cut&Run (Skene and Henikoff, 2017) more so than the ChIC method (Schmid et al., 2004)
- Planning to run the SEACR for the peak calling of the Cut&Run files since it is more sensitive to low background noise that is common in that type of analysis (as opposed to MACS2)
- Need the index bam files (.bam.bai) files which i do not have for most of the deduplicated files
- Incorporated the samtools index to the end of the markdup script (markdup.sh) so that the indexed bam files are added as well 
- Can also just run samtools index on its own now to add the indeces to the folders
- need to transfer the JAN-004 human bowtie aligned bam file to the HPC results section if its in the Janssen project folder
- Created the bam indexing script (bam_indexing.sh) to index the bams by looping over the different folders (tagged, human and drosophila)

02/08/25
- Ran the bam indexing script but seem to not have the results for the tagged bams, will rerun
- Reran and generated the tagged and drososphila bai files (already present for the human bams)
- created script to generate the genome file that needs to be used by the genomecov function (faidx_genome.sh) 
- Running the faidx script revealed that the human genome fasta file is missing many chr records
- Downloading a new genome assembly for the human genome
- Transfered the new assembly (Homo_sapiens.GRCh38.dna.primary_assembly.fa) to the ref_genomes folder
- Will build new ref genome later
- Seem to have the correct genome assembly for the drosophila genome

04/08/25
- Created the fragment_bed.sh script to convert the paired-end BAMs to fragment bedgraphs
- Running now for the first time
- Generated the fragment bedgraphs for the tagged bam files
- Running the bamCoverage.sh script to generate the bigwig files for the tagged bam files 
- Downlaoding the new reference genomes that i need to use from the get go
- Editted the config file structure and content 
- Rerunnig the gneome indexing script (bowtie_index.sh) 

05/08/25
- Edits to the config file and to the faidx script for the unzipping of .gz files
- Rerunning the faidx script for the drosophila genome
- Modifications to flagtat_summary_array (previously genome_cov.sh)
- Rerunning the fragment_bed.sh script for the tagged bam files
- Rerunning the markdup script (with removal) for the tagged bam files since the duplicates were still present in the flagstat results
- Reran the flagstat script for the tagged bam files
- About 2M reads are left after the deduplication for each tagged bam file

07/08/25
- Looking at the flagstat results from teh tagged dedup bams, can see that there are 
    a lot fewer reads thata re mapped correctly from the samples
- The samples tend to have 600k-900k reads mapped correctly with only two samples 
    having 2M reads with highly accurate mapping

08/08/25
- Edit to the markdup script for named output files
- Rerunning the markdup script for the tagged bam files without removal
- Created new config file for looping over instead of naming every folder manually; not in use yet

11/08/25
- added loop over the "marked" "removed" subfolder names to create those folders easier to the "shared_config_2.sh" file
- downloaded SEACR from the github and added it to deeptools environment
- rerunning the flagstat script for the marked deduplicated tagged bam files

12/08/25
- Running SEACR on the removed deduplicated tagged bam files
- Creating the frip script but still missing the output files generation correctly

14/08/25
- Going to try to run the nf-cutandrun pipeline first and then see if individual sections of the pipeline need to be refined based on our experimental setup
- Merging the fastq files before trimming (creating new concat script concat_fastq.sh)
- Running the nf-core cut and run pipeline on the merged fastq files

15/08/25
- the prior runs of the nf-cutandrun were not successful due to resource limits  
- rerunning the nf-cutandrun pipeline

16/08/25
- Rerunning the nf-cutandrun with the human data 

17/08/25
- The bowtie2 alignment tasks take too long in the nf-core cutandrun pipeline 
- Need to find a way to parallelise the alignment portion of the job

18/08/25
- looking at the results from the nf-cutandrun from the drosophila data
- the alignments are very low (<2%)
- the tagged bams (SCMO produced) are of a higher alignmment rate
- Reading up on the SCMO scripts that are run for the ChIC pipeline (snakemake chic and bamtagmultiome.py) 
- since the SMCO tagged alignments have already been deduped removing the folders 
- going back to using the tagged alignment files since i understand them better now
- Rerunning several of the processes: 
    - checked the deduplication rates 
    - generated the bigwig files
    - peak calling with SEACR (relaxed mode, 0,05 FDR, no control)
    - Generated the frip scores for each sample
    - Generated the IGV snapshots at different window sizes
- For SEACR using the non-DSB control as a background in SEACR would incorrectly treat real H3K9ac signal as noise, suppressing true peaks and inflating false negatives.
- Created the FRIP scoring script (frip.sh)
- Ran the FRIP script for the tagged bam files to obtain the FRIP scores for each sample in the frip_results folder

19/08/25
- Writing the diffbinding script for the tagged bam files (run_diffbind.r & Diffbind.sh)
- Running the diffbinding script for the tagged bam files
- Generated the diffbinding results for the tagged bam files
- Showed: 
    - Significant differentially bound (DB) sites (MA plot)
    - Increase in the DBs in the DSB induced samples vs the control (volcano plot)
    - Differentially bound sites are distinct in the DSB induced samples vs the control (heatmap)

21/08/25
- Editted the diffbinding scripts (R and bash) to use flags for outdir, samplesheet and cofactor
- Writing a R script to convert the bed files to a bigwig restricted to the significantly differentially expressed regions
- Rerunning the diffbinding script for the tagged bam files since the last run failed

Next:
- Look at the differentially bound sites for the tagged bam files
- need to re-align the fatqs using the new human ref genome
- need to look at the DRwhite locus for deletions/insertions